<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Classification on Austin David Brown</title>
    <link>https://austindavidbrown.github.io/categories/classification/</link>
    <description>Recent content in Classification on Austin David Brown</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© 2024 — Austin David Brown</copyright>
    <lastBuildDate>Sat, 12 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://austindavidbrown.github.io/categories/classification/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Gradient Boosting in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/gradient-boosting-in-python/</link>
      <pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/gradient-boosting-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ Previously, we constructed regression trees and found that single trees predict sub-optimally. Gradient boosting is a supervised learning algorithm. Originally, boosting was derived as an ensemble method of weak-learners and later Friedman derived gradient boosting in terms of gradient descent in a function space. Friedman&#39;s excellent paper is the interpretation we will follow. We implement a naive gradient boosting algorithm for binary classification using the implementation from XGBoost that was originally published by Friedman.</description>
    </item>
    
    <item>
      <title>Random Forest in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/random-forest-in-python/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/random-forest-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ Previously, we constructed classification and regression trees. We saw that trees are suboptimal predictors on their own. A supervised learning method from someone I admire, Leo Breiman, is the Random forest. The main idea is to construct an ensemble of independent trees to improve the prediction capacity of a single tree. We do this in the following way. We sample $n$ i.i.d. observations $(X_i, Y_i)_{i = 1}^n = \mathcal{D_n}$ to build the training dataset.</description>
    </item>
    
    <item>
      <title>Classification Decision Trees in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/classification-decision-trees-in-python/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/classification-decision-trees-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ Classification decision trees use the binary tree data structure to recursively split the feature space and then fit a weight at each leaf of the tree with a classification prediction. A tree can be represented by $K$ leaf nodes dividing the feature space into regions $(R_k)$ and weights $(w_k)$ so that the tree is \[ T(x) = \sum_{k = 1}^K w_k I(x \in R_k). \] Decision trees are a supervised learning method.</description>
    </item>
    
    <item>
      <title>Multinomial Logistic Regression in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/multinomial-logistic-regression-in-python/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/multinomial-logistic-regression-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ Suppose we have labels $\{ 0, \ldots, K \}$ and wish to count the occupancy of a each label into the vector $y = \{ y_i, \ldots, y_K \} \in \{0, \ldots, n\}^K$ with the constraint that $1^T y = n$. The permutations of $y$ is the multinomial coefficient \[ {n \choose y_1, \ldots, y_K}. \] This is a constrained counting problem. A random variable $Y : \Omega \to \{0, \ldots, n \} ^K$ corrsponding to this constrained counting problem follows a Multinoimal distribution denoted Multinomial(n, K, p) if $1^T Y = n$ and $p$ is a vector on the $K$ dimensional probability simplex so that $p \in [0, 1]^K$ such that $1^T p = 1$ with density \[ P(Y = y) = {n \choose y_1, \ldots, y_K} \prod_{k = 1}^K p_k^{y_k}.</description>
    </item>
    
    <item>
      <title>K-nearest Neighbors in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/k-nearest-neighbors-in-python/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/k-nearest-neighbors-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ The K-nearest neighbors classifier (KNN) is a memory-based classifier. It will be clear what this means later. Let $(X_i, y_i)$ are i.i.d. with distribution $p(x, y) dm$ where $X_i \in \mathbb{R}^d$ are the features and $y \in \{c_1, \ldots, c_m \}$ are the labels. Choose $k \in \mathbb{N}$ and let $\mathcal{D}_n = (X_i, y_i)_{i = 1}^n$ be the training data. We can partition the feature space by $N_k(x, \mathcal{D}_n)$ being the k-nearest points to $x$ using a metric typically chosen as the $l^2$ norm.</description>
    </item>
    
    <item>
      <title>Minimal Single hidden layer feed-forward Neural Network in Python</title>
      <link>https://austindavidbrown.github.io/post/2018/12/minimal-single-hidden-layer-feed-forward-neural-network-in-python/</link>
      <pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2018/12/minimal-single-hidden-layer-feed-forward-neural-network-in-python/</guid>
      <description>The goal of neural networks in general is to approximate some function $f$. The Universal Approximation Theorem says neural networks have the capacity to accomplish this for a large class of functions. In this case, we seek to approximate the function that generates a binary classification problem. We can model this with the binary random variable $Y|X : \Omega \to \{0, 1\}$ with conditional distribution \[ p(x) = P(Y = y | X = x) = P(Y = 1 | X = x)^y (1 - P(Y = 1 | X = x))^{1 - y} \] which we seek to approximate.</description>
    </item>
    
  </channel>
</rss>