<html>
<head>
  <title></title>
  <meta name="author" content="Austin David Brown" />
  <meta name="date" content="2019-01-12" />
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
       displayMath: [['\\[', '\\]']],
       inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"></script>

  <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
  <style>
  body {
    word-wrap: break-word; /* stops words exceeding containers */
    text-size-adjust: 100% !important; /* Fixes mobile messing up text sizes */
  }
  algorithm {
    display: table;
    border: solid 1px;
    padding: 1em;
    margin: 1em;
    white-space: pre;
  }
  </style>
</head>
<body>
<h2>Gradient Boosting with Trees in Python</h2>

$
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
$

Previously, we constructed regression trees and found that single trees predict sub-optimally.
Gradient boosting is a supervised learning algorithm.
Originally, boosting was derived as an ensemble method of weak-learners and later Friedman derived gradient boosting in terms of gradient descent in a function space.
Friedman's excellent paper is the interpretation we will follow.
We implement a naive gradient boosting algorithm for binary classification using the implementation from <a href="https://arxiv.org/abs/1603.02754">XGBoost</a> that was originally published by Friedman.
A softmax implementation would be more complicated. However, one could alternatively fit multiple binary classification gradient boosting algorithms to perform multiple classification.
<br>
<br>

The exposition is derived from the <a href="https://arxiv.org/abs/1603.02754">XGBoost</a> paper.
Let $\mathcal{D}_n = (X_i, Y_i)_{i = 1}^n$ be the i.i.d training sample.
We wish to find an $F \in \mathcal{F}$ that minimizes the loss $L : \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}_{+}$ where $\mathcal{F}$ is the function space of regression trees.
Let $f$ be a tree so that $f(x) = \sum_{l =1}^L w_l I(x \in R_l)$ where partitions $(R_l)$ and weights $(w_l)$ correspond to the number of leaves $L$.

We approximate a gradient step by using the Taylor expansion

\[
L(y, F_k + f) 
\approx L(y, F_k) + g^T f + \frac{1}{2} f^T Hf
\]
where $g$ is the gradient vector and $H$ is the Hessian matrix and solve for the optimial $f$ which yields
\[
f = -H^{-1} g.
\]
Now, we use this approximation to determine the weight computation and the splitting measure to construct the tree $f$.
We fit each leaf with a constant weight and approximate $H$ by a diagonal matrix $diag_i(h_i)$.
Let $I_l$ be the indices for the leaf $l$.
Then the optimal weight in leaf $l$ is
\[
w^*_l = -\frac{\sum_{i \in I_l} g_i}{\sum_{i \in I_l} h_i}
\]
and substituting for the optimal leaf loss yields
\[
L_l^*(y, F + f) = L(y, F_k)  -\frac{1}{2} \frac{ \left( \sum_{i \in I_l} g_i \right)^2 }{\sum_{i \in I_l} h_i}.
\]
Let $I, I_R, I_L$ be the indices of a node and its left and right leaves respectively.
We can ignore the constant term and approximate the change in optimal loss between the parent node and the two leaves by
\[
\frac{ \left( \sum_{i \in I} g_i \right)^2 }{\sum_{i \in I} h_i}
- \frac{ \left( \sum_{i \in I_L} g_i \right)^2 }{\sum_{i \in I_L} h_i}
- \frac{ \left( \sum_{i \in I_R} g_i \right)^2 }{\sum_{i \in I_R} h_i}
\]
This may need to be modified to a weighted average depending on the loss.

Let $f_0$ be an initial approximation to $y$ and $\eta$ be the learning rate.
We iteratively perform gradient descent in the space of regression trees by
\[
L(y, F^{k + 1}(x)) = L\left( y, F^{k}(x) + \eta f^k(x) \right).
\]
The algorithm is
<algorithm><b>Algorithm: Gradient Boost</b>&nbsp;
Choose initial value $f_0$ and learning rate $\eta$.
<b>repeat </b> until convergence
  $f \leftarrow$ Fit a Regression tree to the approximated loss $L(y, f_{k} + f)$&nbsp;
  $f_{k + 1} \leftarrow f_k + \eta f$
</algorithm>
<br>
<br>

For this implementation, we use the 2 class entropy loss with a sigmoid transformation $\sigma: \mathbb{R} \to [0, 1]$:
\[
H(y, F) = \sum_{i = 1}^n y_i \log(\sigma(F_i)) + (1 - y_i) \log(1 - \sigma(F_i)).
\]

The gradient and diagonal Hessian approximation are
\begin{align*}
&g = \sigma(F) - y
\\
&diag(h_i) = diag\left( \sigma(F_i) (1 - \sigma(F_i)) \right).
\end{align*}

<br>
<br>
We put this theory together in a naive implementation with Python and 
using some of the ideas from the structure of the <a href="https://arxiv.org/abs/1603.02754">XGBoost</a> source.
First, we implement a class for the loss function.

<pre class="prettyprint lang-python">
import numpy as np
import matplotlib.pyplot as plt

class BinaryEntropy():
  @staticmethod
  def sigmoid(x):
    return np.exp(x) / (1 + np.exp(x))

  # gradient
  @staticmethod
  def g(y_1, y_2):
    return BinaryEntropy.sigmoid(y_2) - y_1

  # Hessian diagonal
  @staticmethod
  def H_diag(y_1, y_2):
    return BinaryEntropy.sigmoid(y_2) * (1 - BinaryEntropy.sigmoid(y_2))

  @staticmethod
  def compute(y, p_pred):
    return -1/y.shape[0] * (np.sum(y * np.log(p_pred) + (1 - y) * (np.log(1 - p_pred))))

  @staticmethod
  def approx(y, y_pred):
    eps = 10**(-16)
    return -1/2 * 1/max(np.sum(BinaryEntropy.H_diag(y, y_pred)), eps) * np.sum(BinaryEntropy.g(y, y_pred))**2

  @staticmethod
  def compute_weight(y, y_pred):
    eps = 10**(-16)
    return -1/max(np.sum(BinaryEntropy.H_diag(y, y_pred)), eps) * np.sum(BinaryEntropy.g(y, y_pred))

  @staticmethod
  def predTransform(w):
    return BinaryEntropy.sigmoid(w)
</pre>

Next, we modify the previous regression tree code to utilize the new loss class.

<pre class="prettyprint lang-python">
class GBTree():
  @staticmethod
  def split_data(X, y, feature_index, feature_value):
    return {
      "I_left": np.where(X[:, feature_index] <= feature_value)[0],
      "I_right": np.where(X[:, feature_index] > feature_value)[0],
    }

  @staticmethod
  def greedy_best_split(X, y, y_pred, loss):
    best_feature_index = 0
    best_split_value = 0
    best_dloss = 0
    best_split = {
      "I_left": np.array([]),
      "I_right": np.array([]),
    }

    n_features = X.shape[1]
    parent_loss = loss.approx(y, y_pred)
    for feature_index in range(0, n_features):
      split_values = np.unique(X[:, feature_index])
      for split_value in split_values:
        split = GBTree.split_data(X, y, feature_index, split_value)

        # If there is a split
        if split["I_left"].shape[0] > 0 and split["I_right"].shape[0] > 0:
          # Compute loss change and update if the change in loss is the best so far
          dloss = parent_loss - loss.approx(y[split["I_left"]], y_pred[split["I_left"]]) - loss.approx(y[split["I_right"]], y_pred[split["I_right"]])
          if dloss >= best_dloss:
            best_feature_index = feature_index
            best_split_value = split_value
            best_split = split
            best_dloss = dloss
    return best_dloss, best_feature_index, best_split_value, best_split

  @staticmethod
  def fit_tree(X, y, y_pred, loss, depth = 1, 
               max_depth = 100, tolerance = 10**(-3)):
    node = {}

    # If we can split, find the best split by greedy algorithm
    if y.shape[0] >= 2:
      dloss, feature_index, split_value, split = GBTree.greedy_best_split(X, y, y_pred, loss)
      node["dloss"] = dloss
      node["feature_index"] = feature_index
      node["split_value"] = split_value

      # If there is a split and the stopping criterion is not met, branch 2 leaves
      if split["I_left"].shape[0] > 0 and split["I_right"].shape[0] > 0 and dloss >= tolerance and depth < max_depth:
        node["left"] = GBTree.fit_tree(X[split["I_left"]], y[split["I_left"]], y_pred[split["I_left"]], loss,
                                     depth = depth + 1, max_depth = max_depth, tolerance = tolerance)
        node["right"] = GBTree.fit_tree(X[split["I_right"]], y[split["I_right"]], y_pred[split["I_right"]], loss,
                                      depth = depth + 1, max_depth = max_depth, tolerance = tolerance) 
      # Terminal node
      else:
        node["w"] = loss.compute_weight(y, y_pred)
        node["left"] = None
        node["right"] = None
    # Terminal node    
    else:
      node["w"] = loss.compute_weight(y, y_pred)
      node["left"] = None
      node["right"] = None
    return node

  @staticmethod
  def predict_one(node, x):
    if node["left"] == None or node["right"] == None:
      return node["w"]
    else:
      if x[node["feature_index"]] <= node["split_value"]:
        return GBTree.predict_one(node["left"], x)
      else:
        return GBTree.predict_one(node["right"], x)

  @staticmethod
  def predict(node, loss, X):
    n_samples = X.shape[0]
    predictions = np.zeros(n_samples)
    for i in range(0, n_samples):
      predictions[i] = GBTree.predict_one(node, X[i])
    return predictions

  @staticmethod
  def print_tree(node, depth = 0):
    if node["left"] == None or node["right"] == None:
      print(f'{depth * "  "}weight: {node["w"]}')
    else:
      print(f'{depth * "  "}X{node["feature_index"]} <= {node["split_value"]}')
      GBTree.print_tree(node["left"], depth + 1)
      GBTree.print_tree(node["right"], depth + 1)
</pre>

Finally, we construct the gradient boosting class.

<pre class="prettyprint lang-python">
class GradientBoost():
  def __init__(self, loss, learning_rate = 1, max_depth = 10, tolerance = 10**(-3), max_iter = 10):
    self.trees = []
    self.loss = loss
    self.learning_rate = learning_rate
    self.max_depth = max_depth
    self.tolerance = tolerance
    self.max_iter = max_iter

  def train(self, X, y):
    y_pred = np.zeros(y.shape[0])

    for i in range(0, self.max_iter):
      tree = GBTree.fit_tree(X, y, y_pred, self.loss,
                           max_depth = self.max_depth, tolerance = self.tolerance)
      self.trees.append(tree)
      y_pred = self.learning_rate * GBTree.predict(tree, self.loss, X)

      print("Training Loss:", self.loss.compute(y, self.predict(X)))

  def predict(self, X):
    n_samples = X.shape[0]
    
    y_pred = np.zeros(n_samples)
    for i in range(0, len(self.trees)):
      y_pred += self.learning_rate * GBTree.predict(self.trees[i], self.loss, X) 
    return self.loss.predTransform(y_pred)
</pre>

We use the <a href="http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits">UCI optical handwritten digits dataset</a>. Only the test set is used and the classification is reduced to only the 1's digit. Finally, we split this dataset into a training and test set.
Here is a plot of some digits for reference.

<center>
<img height="300" src="data:image/png;base64,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">
</center>
<br/>

<pre class="prettyprint lang-python">
test = np.loadtxt("data/optdigits_test.txt", delimiter = ",")
X = test[:, 0:64]
y = test[:, 64]

# Convert to binary classification
y[y != 1] = 0

# Train/test split
n_samples = X.shape[0]
n_TRAIN = int(.75 * n_samples)
I = np.arange(0, n_samples)
TRAIN = np.random.choice(I, n_TRAIN, replace = False)
TEST = np.setdiff1d(I, TRAIN)
X_train = X[TRAIN, :]
y_train = y[TRAIN]
X_test = X[TEST, :]
y_test = y[TEST]
</pre>

We train the gradient boosting classifier with the binary entropy loss class and report the prediction accuracy on the training and test sets respectively.

<pre class="prettyprint lang-python">
gb = GradientBoost(loss = BinaryEntropy())
gb.train(X_train, y_train)

print("Train accuracy:", 1/X_train.shape[0] * np.sum((np.round(gb.predict(X_train)) == y_train).astype(int)))
print("Test accuracy", 1/X_test.shape[0] * np.sum((np.round(gb.predict(X_test)) == y_test).astype(int)))
</pre>

<pre>
Training Loss: 0.1269280110429725
Training Loss: 0.04256623711865767
Training Loss: 0.011534388597255378
Training Loss: 0.003263137623967758
Training Loss: 0.0009067682950024066
Training Loss: 0.0002528152405255996
Training Loss: 7.038866445512709e-05
Training Loss: 1.9602627915503846e-05
Training Loss: 5.458568180889396e-06
Training Loss: 1.5200290017234747e-06
Train accuracy: 1.0
Test accuracy 0.9866666666666667
</pre>


<h2>References.</h2>
http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits
<br>
<br>
Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting System. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16). ACM, New York, NY, USA, 785-794. DOI: https://doi.org/10.1145/2939672.2939785
<br>
<br>
Friedman, Jerome H., Trevor J. Hastie and Robert Tibshirani. “Additive Logistic Regression : a Statistical View of Boosting.” (1998).
<br>
<br>
Jerome H. Friedman. 2002. Stochastic gradient boosting. Comput. Stat. Data Anal. 38, 4 (February 2002), 367-378. DOI=http://dx.doi.org/10.1016/S0167-9473(01)00065-2

<!--  Twitter -->
<br/>
<br/>
<hr/>
<a class="twitter-timeline" href="https://twitter.com/austindavbrown?ref_src=twsrc%5Etfw">Tweets by austindavbrown</a>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</body>
</html>