<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Austin David Brown</title>
    <link>https://austindavidbrown.github.io/post/</link>
    <description>Recent content in Posts on Austin David Brown</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© 2024 — Austin David Brown</copyright>
    <lastBuildDate>Wed, 05 Jun 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://austindavidbrown.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Batch Means for Variance Estimation in Langevin Monte Carlo</title>
      <link>https://austindavidbrown.github.io/post/2019/06/batch-means-for-variance-estimation-in-langevin-monte-carlo/</link>
      <pubDate>Wed, 05 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/06/batch-means-for-variance-estimation-in-langevin-monte-carlo/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} \newcommand{\ip}[2]{\left\langle#1,#2\right\rangle} \newcommand{\e}{\epsilon} \newcommand{\R}{\mathbb{R}} \newcommand{\F}{\mathcal{F}} \let\phi\varphi $ The motivation here is that it is difficult to know if the Markov chain $(X_k)$ from an MCMC algorithm is any good. The batch means approach is based upon the functional CLT from Kipnis and Varadhan. Theorem (Functional Markov Chain CLT).   Let $P = p^{\otimes n}$ be an irreducible, reversible Markov chain on $\R^d$ with initial distribution $\nu$ and stationary distribution $\pi$.</description>
    </item>
    
    <item>
      <title>Brownian Motion and the Heat Equation</title>
      <link>https://austindavidbrown.github.io/post/2019/06/brownian-motion-and-the-heat-equation/</link>
      <pubDate>Mon, 03 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/06/brownian-motion-and-the-heat-equation/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} \newcommand{\ip}[2]{\left\langle#1,#2\right\rangle} \newcommand{\e}{\epsilon} \newcommand{\R}{\mathbb{R}} \newcommand{\F}{\mathcal{F}} \let\phi\varphi $ The motivation is that we want to solve the PDE called the heat equation where the time change is the average change in space. The Laplacian $\Delta$ models &#34;average change&#34; in space. Thus, the heat equation is \[ \partial_t u_t(x) = \Delta u_t(x) \] with initial value $u_0(x) = u(x)$. The Laplacian $\Delta$ models &#34;average change&#34;. Consider a symmetric random walk with density $p_t(x)$ with time $t$ in discrete steps.</description>
    </item>
    
    <item>
      <title>Geometric Convergence of Langevin Monte Carlo</title>
      <link>https://austindavidbrown.github.io/post/2019/05/geometric-convergence-of-langevin-monte-carlo/</link>
      <pubDate>Wed, 29 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/05/geometric-convergence-of-langevin-monte-carlo/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} \newcommand{\e}{\epsilon} \newcommand{\R}{\mathbb{R}} \let\phi\varphi $ Let $(P_t)$ be the Langevin diffusion semigroup for the invariant distribution $\Gamma$ where $d\Gamma = \gamma dm = \frac{1}{\int_{\mathbb{R^d}}\exp(-V)dm}\exp(-V)dm$ with $m$ being d-dimensional Lebesgue measure. The Markov chain for the Euler discretization is \[ X_{k + 1} = X_k - h \nabla V(X_k) + \sqrt{2h} Z \] where $Z$ is standard normal and $h$ is sufficiently small. The question is does an invariant distribution exist for $(X_k)$ and how fast is the convergence?</description>
    </item>
    
    <item>
      <title>Controlling Errors in Langevin Monte Carlo</title>
      <link>https://austindavidbrown.github.io/post/2019/05/controlling-errors-in-langevin-monte-carlo/</link>
      <pubDate>Sun, 26 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/05/controlling-errors-in-langevin-monte-carlo/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ The problem is that we want to compute $\Gamma f = \int f d\Gamma$ where $d\Gamma = \gamma dm = \frac{1}{\int_{\mathbb{R^d}}\exp(-V)dm}\exp(-V)dm$ with $m$ being d-dimensional Lebesgue measure and for some class of functions $f : \mathbb{R}^d \to \mathbb{R}$ with $f \in \mathcal{F}$. But we cannot do this, so we approximate $\pi f$ using samples $(X_k)_{k = 1}^n$ and use \[ \Gamma_n f = \frac{1}{n} \sum_{k = 0}^{n - 1} f(X_k).</description>
    </item>
    
    <item>
      <title>K-fold Cross-Validation in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/k-fold-cross-validation-in-python/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/k-fold-cross-validation-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ K-fold cross-validation has many different formulations and is mainly used for 2 things:  Comparing learning algorithms Hyper-parameter tuning for a single learning algorithm  The goal is to approximate the prediction error of a learning algorithm. The main idea is to construct a K-partition of the data so that a method&#39;s prediction is evaluated on a single partition but trained on all remaining partitions. Typically, $K$ is chosen to be $5$ or $10$ and there is no general theoretical best $K$ to choose.</description>
    </item>
    
    <item>
      <title>Gradient Boosting in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/gradient-boosting-in-python/</link>
      <pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/gradient-boosting-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ Previously, we constructed regression trees and found that single trees predict sub-optimally. Gradient boosting is a supervised learning algorithm. Originally, boosting was derived as an ensemble method of weak-learners and later Friedman derived gradient boosting in terms of gradient descent in a function space. Friedman&#39;s excellent paper is the interpretation we will follow. We implement a naive gradient boosting algorithm for binary classification using the implementation from XGBoost that was originally published by Friedman.</description>
    </item>
    
    <item>
      <title>Random Forest in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/random-forest-in-python/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/random-forest-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ Previously, we constructed classification and regression trees. We saw that trees are suboptimal predictors on their own. A supervised learning method from someone I admire, Leo Breiman, is the Random forest. The main idea is to construct an ensemble of independent trees to improve the prediction capacity of a single tree. We do this in the following way. We sample $n$ i.i.d. observations $(X_i, Y_i)_{i = 1}^n = \mathcal{D_n}$ to build the training dataset.</description>
    </item>
    
    <item>
      <title>Regression Decision Trees in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/regression-decision-trees-in-python/</link>
      <pubDate>Fri, 11 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/regression-decision-trees-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ Regression decision trees are constructed in the same manor as classification decision trees. These trees use a binary tree to recursively divide the feature space fitting a weight at each terminal node of the tree. A tree $T$ has the form \[ T(x) = \sum_{k = 1}^K w_k I(x \in R_k). \] where $K$ is the number of terminal nodes, $(R_k)$ is the partitions of the feature space, and $(w_k)$ are the weights at each terminal node.</description>
    </item>
    
    <item>
      <title>Classification Decision Trees in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/classification-decision-trees-in-python/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/classification-decision-trees-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ Classification decision trees use the binary tree data structure to recursively split the feature space and then fit a weight at each leaf of the tree with a classification prediction. A tree can be represented by $K$ leaf nodes dividing the feature space into regions $(R_k)$ and weights $(w_k)$ so that the tree is \[ T(x) = \sum_{k = 1}^K w_k I(x \in R_k). \] Decision trees are a supervised learning method.</description>
    </item>
    
    <item>
      <title>Markov Chain Monte Carlo Sampling in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/markov-chain-monte-carlo-sampling-in-python/</link>
      <pubDate>Wed, 09 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/markov-chain-monte-carlo-sampling-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ It is common in machine learning that we desire to compute an integral such as \[ \int_{x \in \mathbb{R}^d} f(x) p(x) dx. \] where $f : \mathbb{R}^d \to \mathbb{R}$, $f \in L_1(pdm)$, and $p$ is a probability density. Both Lebesgue and Riemann integrals are extremely difficult to evaluate on a computer. This is because the number of grid points needed grows exponentially in the dimension. One approach is to use Monte-Carlo sampling.</description>
    </item>
    
    <item>
      <title>Multinomial Logistic Regression in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/multinomial-logistic-regression-in-python/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/multinomial-logistic-regression-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ Suppose we have labels $\{ 0, \ldots, K \}$ and wish to count the occupancy of a each label into the vector $y = \{ y_i, \ldots, y_K \} \in \{0, \ldots, n\}^K$ with the constraint that $1^T y = n$. The permutations of $y$ is the multinomial coefficient \[ {n \choose y_1, \ldots, y_K}. \] This is a constrained counting problem. A random variable $Y : \Omega \to \{0, \ldots, n \} ^K$ corrsponding to this constrained counting problem follows a Multinoimal distribution denoted Multinomial(n, K, p) if $1^T Y = n$ and $p$ is a vector on the $K$ dimensional probability simplex so that $p \in [0, 1]^K$ such that $1^T p = 1$ with density \[ P(Y = y) = {n \choose y_1, \ldots, y_K} \prod_{k = 1}^K p_k^{y_k}.</description>
    </item>
    
    <item>
      <title>K-means in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/k-means-in-python/</link>
      <pubDate>Sat, 05 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/k-means-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ The K-means problem is an unsupervised learning problem. Let $(x_i)_{i = 1}^n$ where $x_i \in \mathbb{R}^d$ be the unlabeled data. Assume the number of clusters $k$ is known. This is a huge assumption. The most popular algorithm is by Lloyd. Algorithm: Lloyds K-means Choose initial cluster centers $\mu_1, \ldots, \mu_K$. Repeat until stopping criterion is met: For each $x_i$, compute the closest center and classify: $c_i = \text{argmin}_{k}\norm{x_i - \mu_k}$ Update each center: $\mu_j \leftarrow \frac{1}{N_k}\sum_{i : c_i = k} x_i$  This is a greedy algorithm and can be thought of as the EM algorithm on a spherical Gaussian mixture model.</description>
    </item>
    
    <item>
      <title>K-nearest Neighbors in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/k-nearest-neighbors-in-python/</link>
      <pubDate>Thu, 03 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/k-nearest-neighbors-in-python/</guid>
      <description>$ \newcommand{\norm}[1]{\left\lVert#1\right\rVert} $ The K-nearest neighbors classifier (KNN) is a memory-based classifier. It will be clear what this means later. Let $(X_i, y_i)$ are i.i.d. with distribution $p(x, y) dm$ where $X_i \in \mathbb{R}^d$ are the features and $y \in \{c_1, \ldots, c_m \}$ are the labels. Choose $k \in \mathbb{N}$ and let $\mathcal{D}_n = (X_i, y_i)_{i = 1}^n$ be the training data. We can partition the feature space by $N_k(x, \mathcal{D}_n)$ being the k-nearest points to $x$ using a metric typically chosen as the $l^2$ norm.</description>
    </item>
    
    <item>
      <title>PCA in Python</title>
      <link>https://austindavidbrown.github.io/post/2019/01/pca-in-python/</link>
      <pubDate>Wed, 02 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2019/01/pca-in-python/</guid>
      <description>Principal Components Analysis is an unsupervised learning or lossy compression method. Let $X = (X_i)_{i =1}^n$ such that $X_i \in \mathbb{R}^p$ be the data where $n$ is the number of observations and $p$ is the number of features. The sample covariance matrix with respect to the features is \[ \frac{1}{n} (X - \bar X)^T (X - \bar X) \] where the mean is with respect to the rows. This gives us all of the variance of covariance information of the features.</description>
    </item>
    
    <item>
      <title>Minimal Single hidden layer feed-forward Neural Network in Python</title>
      <link>https://austindavidbrown.github.io/post/2018/12/minimal-single-hidden-layer-feed-forward-neural-network-in-python/</link>
      <pubDate>Mon, 31 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://austindavidbrown.github.io/post/2018/12/minimal-single-hidden-layer-feed-forward-neural-network-in-python/</guid>
      <description>The goal of neural networks in general is to approximate some function $f$. The Universal Approximation Theorem says neural networks have the capacity to accomplish this for a large class of functions. In this case, we seek to approximate the function that generates a binary classification problem. We can model this with the binary random variable $Y|X : \Omega \to \{0, 1\}$ with conditional distribution \[ p(x) = P(Y = y | X = x) = P(Y = 1 | X = x)^y (1 - P(Y = 1 | X = x))^{1 - y} \] which we seek to approximate.</description>
    </item>
    
  </channel>
</rss>